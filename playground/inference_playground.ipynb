{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/temporal-shift-module/online_demo')\n",
    "\n",
    "#from mobilenet_v2_tsm_test import MobileNetV2\n",
    "from arch_mobilenetv2 import MobileNetV2\n",
    "\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GroupScale(object):\n",
    "    \"\"\" Rescales the input PIL.Image to the given 'size'.\n",
    "    'size' will be the size of the smaller edge.\n",
    "    For example, if height > width, then image will be\n",
    "    rescaled to (size * height / width, size)\n",
    "    size: size of the smaller edge\n",
    "    interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        self.worker = torchvision.transforms.Scale(size, interpolation)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "\n",
    "\n",
    "\n",
    "class GroupCenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.worker = torchvision.transforms.CenterCrop(size)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "\n",
    "\n",
    "\n",
    "class Stack(object):\n",
    "\n",
    "    def __init__(self, roll=False):\n",
    "        self.roll = roll\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        if img_group[0].mode == 'L':\n",
    "            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n",
    "        \n",
    "        elif img_group[0].mode == 'RGB':\n",
    "            if self.roll:\n",
    "                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n",
    "            else:\n",
    "                return np.concatenate(img_group, axis=2)\n",
    "\n",
    "\n",
    "\n",
    "class ToTorchFormatTensor(object):\n",
    "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n",
    "    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n",
    "\n",
    "    def __init__(self, div=True):\n",
    "        self.div = div\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n",
    "        else:\n",
    "            # handle PIL Image\n",
    "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n",
    "            # put it from HWC to CHW format\n",
    "            # yikes, this transpose takes 80% of the loading time/CPU\n",
    "            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "        return img.float().div(255) if self.div else img.float()\n",
    "\n",
    "\n",
    "\n",
    "class GroupNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        rep_mean = self.mean * (tensor.size()[0] // len(self.mean))\n",
    "        rep_std = self.std * (tensor.size()[0] // len(self.std))\n",
    "\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, rep_mean, rep_std):\n",
    "            t.sub_(m).div_(s)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def process_output(idx_, history, num_classes):\n",
    "    # idx_: the output of current frame\n",
    "    # history: a list containing the history of predictions\n",
    "    if not REFINE_OUTPUT:\n",
    "        return idx_, history\n",
    "\n",
    "    max_hist_len = int((20/27)*num_classes) # max history buffer\n",
    "\n",
    "    # mask out illegal action\n",
    "    \n",
    "    if num_classes == 27:\n",
    "        if idx_ in [7, 8, 21, 22, 1, 3]:\n",
    "            idx_ = history[-1]\n",
    "\n",
    "        if idx_ == 0:\n",
    "            idx_ = 2\n",
    "\n",
    "    # use only single no action class\n",
    "    elif num_classes == 3: \n",
    "        if idx_ in [2]:\n",
    "            idx_ = history[-1]\n",
    "        \n",
    "        if idx_ == 0:\n",
    "            idx_ = 0\n",
    "    \n",
    "    # history smoothing\n",
    "\n",
    "    if idx_ != history[-1] and len(history) != 1:\n",
    "        if not (history[-1] == history[-2]): #  and history[-2] == history[-3]):\n",
    "            idx_ = history[-1]\n",
    "    \n",
    "\n",
    "    history.append(idx_)\n",
    "    history = history[-max_hist_len:]\n",
    "\n",
    "    return history[-1], history\n",
    "\n",
    "\n",
    "def get_categories(num_classes):\n",
    "\n",
    "    if num_classes == 27:\n",
    "        catigories = [\n",
    "        \"Doing other things\",  # 0\n",
    "        \"Drumming Fingers\",  # 1\n",
    "        \"No gesture\",  # 2\n",
    "        \"Pulling Hand In\",  # 3\n",
    "        \"Pulling Two Fingers In\",  # 4\n",
    "        \"Pushing Hand Away\",  # 5\n",
    "        \"Pushing Two Fingers Away\",  # 6\n",
    "        \"Rolling Hand Backward\",  # 7\n",
    "        \"Rolling Hand Forward\",  # 8\n",
    "        \"Shaking Hand\",  # 9\n",
    "        \"Sliding Two Fingers Down\",  # 10\n",
    "        \"Sliding Two Fingers Left\",  # 11\n",
    "        \"Sliding Two Fingers Right\",  # 12\n",
    "        \"Sliding Two Fingers Up\",  # 13\n",
    "        \"Stop Sign\",  # 14\n",
    "        \"Swiping Down\",  # 15\n",
    "        \"Swiping Left\",  # 16\n",
    "        \"Swiping Right\",  # 17\n",
    "        \"Swiping Up\",  # 18\n",
    "        \"Thumb Down\",  # 19\n",
    "        \"Thumb Up\",  # 20\n",
    "        \"Turning Hand Clockwise\",  # 21\n",
    "        \"Turning Hand Counterclockwise\",  # 22\n",
    "        \"Zooming In With Full Hand\",  # 23\n",
    "        \"Zooming In With Two Fingers\",  # 24\n",
    "        \"Zooming Out With Full Hand\",  # 25\n",
    "        \"Zooming Out With Two Fingers\"  # 26\n",
    "    ]\n",
    "\n",
    "    elif num_classes == 9: \n",
    "\n",
    "        catigories = [\"Fall\", \"SalsaSpin\", \"Taichi\", \"WallPushups\", \"WritingOnBoard\", \"Archery\", \"Hulahoop\", \"Nunchucks\", \"WalkingWithDog\"]\n",
    "    \n",
    "    elif num_classes == 10:\n",
    "\n",
    "        catigories = [\"N/A\", \"Fall\", \"SalsaSpin\", \"Taichi\", \"WallPushups\", \"WritingOnBoard\", \"Archery\", \"Hulahoop\", \"Nunchucks\", \"WalkingWithDog\"]\n",
    "\n",
    "    elif num_classes == 3 :\n",
    "\n",
    "        catigories = ['N/A', 'Fall', \"Not Fall\"]\n",
    "\n",
    "    elif num_classes == 2:\n",
    "\n",
    "        catigories = [\"Fall\", \"Not Fall\"]\n",
    "\n",
    "    return catigories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOFTMAX_THRES = 0\n",
    "HISTORY_LOGIT = True\n",
    "REFINE_OUTPUT = True\n",
    "WINDOW_NAME = \"GESTURE CAPTURE\"\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "if num_classes not in [2, 3, 9, 10, 27]:\n",
    "    Print(\"Can only handle 2, 10, and 27 classes\")\n",
    "    exit()\n",
    "\n",
    "else:\n",
    "    catigories = get_categories(num_classes)\n",
    "\n",
    "cropping = torchvision.transforms.Compose([\n",
    "    GroupScale(256),\n",
    "    GroupCenterCrop(224),\n",
    "])\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "    cropping,\n",
    "    Stack(roll=False),\n",
    "    ToTorchFormatTensor(div=True),\n",
    "    GroupNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "from torch import nn\n",
    "torch_module = MobileNetV2(n_class=num_classes)\n",
    "#print(torch_module.state_dict().keys())\n",
    "\n",
    "\n",
    "if num_classes == 27:\n",
    "    if not os.path.exists(\"mobilenetv2_jester_online.pth.tar\"):  # checkpoint not downloaded\n",
    "        print('Downloading PyTorch checkpoint...')\n",
    "        url = 'https://hanlab.mit.edu/projects/tsm/models/mobilenetv2_jester_online.pth.tar'\n",
    "        urllib.request.urlretrieve(url, './mobilenetv2_jester_online.pth.tar')\n",
    "\n",
    "\n",
    "    torch_module.load_state_dict(torch.load(\"mobilenetv2_jester_online.pth.tar\"))\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    if num_classes == 9 or num_classes == 10:\n",
    "        #KH model_new = torch.load(\"../../pretrained/9cat/ckpt.best.pth.tar\")\n",
    "        model_new = torch.load(\"/data/w251fall/checkpoints/9_Categories/TSM_w251fall_RGB_mobilenetv2_shift8_blockres_avg_segment8_e50/ckpt.best.pth.tar\")\n",
    "\n",
    "    elif num_classes == 2 or num_classes == 3:\n",
    "        #KH model_new = torch.load(\"../../pretrained/2cat/ckpt.best.pth.tar\")\n",
    "        #model_new = torch.load(\"/data/w251fall/checkpoints/2_Categories/1_TSM_w251fall_RGB_mobilenetv2_shift8_blockres_avg_segment8_e25/ckpt.best.pth.tar\")\n",
    "        model_new = torch.load(\"/data/w251fall/checkpoints/2_Categories/TSM_w251fall_RGB_mobilenetv2_shift8_blockres_avg_segment8_e5/ckpt.best.pth.tar\")\n",
    "\n",
    "\n",
    "    # Fixing new model parameter mis-match\n",
    "    state_dict = model_new['state_dict']\n",
    "    #print(state_dict.keys())\n",
    "\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        #name = k[7:] # remove `module.`\n",
    "\n",
    "        if \"module.base_model.\" in k:\n",
    "            name = k.replace(\"module.base_model.\", \"\")\n",
    "\n",
    "            if \".net\" in name:\n",
    "                name = name.replace(\".net\", \"\")\n",
    "\n",
    "\n",
    "        elif \"module.\" in k:\n",
    "            name = k.replace(\"module.new_fc.\", \"classifier.\")\n",
    "\n",
    "\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "    # load params\n",
    "    torch_module.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "torch_module.eval()\n",
    "\n",
    "\n",
    "def test_video(video_images_dir):\n",
    "    results = [0] * num_classes\n",
    "    \n",
    "    # Load jpgs file names fully qualified\n",
    "    jpg_filenames = [video_images_dir + '/' + s for s in os.listdir(video_images_dir)]\n",
    "\n",
    "\n",
    "\n",
    "    shift_buffer = [torch.zeros([1, 3, 56, 56]),\n",
    "                    torch.zeros([1, 4, 28, 28]),\n",
    "                    torch.zeros([1, 4, 28, 28]),\n",
    "                    torch.zeros([1, 8, 14, 14]),\n",
    "                    torch.zeros([1, 8, 14, 14]),\n",
    "                    torch.zeros([1, 8, 14, 14]),\n",
    "                    torch.zeros([1, 12, 14, 14]),\n",
    "                    torch.zeros([1, 12, 14, 14]),\n",
    "                    torch.zeros([1, 20, 7, 7]),\n",
    "                    torch.zeros([1, 20, 7, 7])]\n",
    "\n",
    "\n",
    "    t = None    \n",
    "    index = 0\n",
    "    idx = 0\n",
    "    history = [2]\n",
    "    history_logit = []\n",
    "    history_timing = []\n",
    "    i_frame = -1\n",
    "\n",
    "    for jpg_filename in jpg_filenames:\n",
    "        \n",
    "        img = cv2.imread(jpg_filename)\n",
    "        \n",
    "        i_frame += 1\n",
    "        #KH _, img = cap.read()  # (480, 640, 3) 0 ~ 255\n",
    "\n",
    "        if i_frame % 1 == 0:\n",
    "            t1 = time.time()\n",
    "            img_tran = transform([Image.fromarray(img).convert('RGB')])\n",
    "            input_var = torch.autograd.Variable(img_tran.view(1, 3, img_tran.size(1), img_tran.size(2)))\n",
    "\n",
    "            #prediction = torch_module(input_var, *shift_buffer) #demo mobilenet\n",
    "            prediction = torch_module(input_var) #arch mobilenet\n",
    "\n",
    "\n",
    "            feat, shift_buffer = prediction[0], prediction[1:]\n",
    "\n",
    "\n",
    "            if SOFTMAX_THRES > 0:\n",
    "\n",
    "                feat_np = feat.detach().numpy().reshape(-1)\n",
    "                feat_np -= feat_np.max()\n",
    "\n",
    "                softmax = np.exp(feat_np) / np.sum(np.exp(feat_np))\n",
    "\n",
    "                #KH print(max(softmax))\n",
    "        \n",
    "                if max(softmax) > SOFTMAX_THRES:\n",
    "                    idx_ = np.argmax(feat.detach().numpy(), axis=1)[0]\n",
    "        \n",
    "                else:\n",
    "                    idx_ = idx\n",
    "    \n",
    "            else:\n",
    "                #KH print(feat.detach().numpy())\n",
    "                #idx_ = np.argmax(feat.detach().numpy(), axis=1)[0] For demo mobilenet\n",
    "                idx_ = np.argmax(feat.detach().numpy()) # For archnet mobilenet\n",
    "\n",
    "\n",
    "            if HISTORY_LOGIT:\n",
    "                history_logit.append(feat.detach().numpy())\n",
    "                history_logit = history_logit[-int(12/27*num_classes):]\n",
    "                avg_logit = sum(history_logit)\n",
    "                #idx_ = np.argmax(avg_logit, axis=1)[0] For demo mobilenet\n",
    "                idx_ = np.argmax(avg_logit)  #For archnet mobilenet\n",
    "\n",
    "            idx, history = process_output(idx_, history, num_classes)\n",
    "            \n",
    "\n",
    "            t2 = time.time()\n",
    "#             print(f\"Final {index} Attempt {catigories[idx]}\")\n",
    "            results[idx] = results[idx] + 1\n",
    "\n",
    "            \n",
    "            current_time = t2 - t1\n",
    "\n",
    "        \n",
    "#         img = cv2.resize(img, (640, 480))\n",
    "#         img = img[:, ::-1]\n",
    "#         height, width, _ = img.shape\n",
    "#         label = np.zeros([height // 10, width, 3]).astype('uint8') + 255\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: /data/w251fall/jpg/Fall/fall-25-front-urfall.val\n",
      "Testing: /data/w251fall/jpg/Fall/steph_2682_(1).train\n",
      "Testing: /data/w251fall/jpg/Fall/ten_0002_(10).train\n",
      "Testing: /data/w251fall/jpg/NotFall/v_HulaHoop_g01_c01.val\n",
      "Testing: /data/w251fall/jpg/NotFall/v_HulaHoop_g02_c01.val\n",
      "Testing: /data/w251fall/jpg/NotFall/v_TaiChi_g23_c01.train\n",
      "Testing: /data/w251fall/jpg/NotFall/v_Archery_g25_c07.train\n",
      "Testing: /data/w251fall/jpg/NotFall/v_Nunchucks_g01_c01.val\n",
      "Testing: /data/w251fall/jpg/NotFall/v_Nunchucks_g02_c06.val\n",
      "Testing: /data/w251fall/jpg/NotFall/v_WritingOnBoard_g05_c02.val\n",
      "Testing: /data/w251fall/jpg/NotFall/v_WritingOnBoard_g07_c06.val\n",
      "Testing: /data/w251fall/jpg/NotFall/v_Archery_g02_c07.val\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Video                                                  </td><td>N/A</td><td>Fall</td><td>Not Fall</td></tr>\n",
       "<tr><td>/data/w251fall/jpg/Fall/fall-25-front-urfall.val       </td><td>0  </td><td>85  </td><td>0       </td></tr>\n",
       "<tr><td>/data/w251fall/jpg/Fall/steph_2682_(1).train           </td><td>0  </td><td>104 </td><td>0       </td></tr>\n",
       "<tr><td>/data/w251fall/jpg/Fall/ten_0002_(10).train            </td><td>0  </td><td>53  </td><td>2       </td></tr>\n",
       "<tr><td>/data/w251fall/jpg/NotFall/v_HulaHoop_g01_c01.val      </td><td>0  </td><td>0   </td><td>91      </td></tr>\n",
       "<tr><td>/data/w251fall/jpg/NotFall/v_HulaHoop_g02_c01.val      </td><td>0  </td><td>0   </td><td>111     </td></tr>\n",
       "<tr><td>/data/w251fall/jpg/NotFall/v_TaiChi_g23_c01.train      </td><td>0  </td><td>0   </td><td>231     </td></tr>\n",
       "<tr><td>/data/w251fall/jpg/NotFall/v_Archery_g25_c07.train     </td><td>0  </td><td>0   </td><td>134     </td></tr>\n",
       "<tr><td>/data/w251fall/jpg/NotFall/v_Nunchucks_g01_c01.val     </td><td>0  </td><td>170 </td><td>95      </td></tr>\n",
       "<tr><td>/data/w251fall/jpg/NotFall/v_Nunchucks_g02_c06.val     </td><td>0  </td><td>0   </td><td>297     </td></tr>\n",
       "<tr><td>/data/w251fall/jpg/NotFall/v_WritingOnBoard_g05_c02.val</td><td>0  </td><td>0   </td><td>98      </td></tr>\n",
       "<tr><td>/data/w251fall/jpg/NotFall/v_WritingOnBoard_g07_c06.val</td><td>0  </td><td>0   </td><td>102     </td></tr>\n",
       "<tr><td>/data/w251fall/jpg/NotFall/v_Archery_g02_c07.val       </td><td>0  </td><td>0   </td><td>166     </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "\n",
    "\n",
    "test_videos_dirs = [\n",
    "    # Falls\n",
    "    '/data/w251fall/jpg/Fall/fall-25-front-urfall.val',\n",
    "    '/data/w251fall/jpg/Fall/steph_2682_(1).train',\n",
    "    '/data/w251fall/jpg/Fall/ten_0002_(10).train',\n",
    "\n",
    "    # Not Falls\n",
    "    '/data/w251fall/jpg/NotFall/v_HulaHoop_g01_c01.val' ,\n",
    "    '/data/w251fall/jpg/NotFall/v_HulaHoop_g02_c01.val',\n",
    "    '/data/w251fall/jpg/NotFall/v_TaiChi_g23_c01.train',\n",
    "    '/data/w251fall/jpg/NotFall/v_Archery_g25_c07.train',\n",
    "    '/data/w251fall/jpg/NotFall/v_Nunchucks_g01_c01.val',\n",
    "    '/data/w251fall/jpg/NotFall/v_Nunchucks_g02_c06.val',\n",
    "    '/data/w251fall/jpg/NotFall/v_WritingOnBoard_g05_c02.val',\n",
    "    '/data/w251fall/jpg/NotFall/v_WritingOnBoard_g07_c06.val',\n",
    "    '/data/w251fall/jpg/NotFall/v_Archery_g02_c07.val',\n",
    "]\n",
    "\n",
    "output = []\n",
    "output.append([\"Video\"] + catigories)\n",
    "#output.append(test_video(video_images_dir))\n",
    "\n",
    "for vid_dir in test_videos_dirs:\n",
    "    print(\"Testing: {}\".format(vid_dir))\n",
    "    output.append([vid_dir] + test_video(vid_dir))\n",
    "\n",
    "display(HTML(tabulate.tabulate(output, tablefmt='html')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
