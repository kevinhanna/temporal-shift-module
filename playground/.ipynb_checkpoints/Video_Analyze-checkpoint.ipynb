{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/temporal-shift-module/online_demo')\n",
    "\n",
    "#from mobilenet_v2_tsm_test import MobileNetV2\n",
    "from arch_mobilenetv2 import MobileNetV2\n",
    "\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting... \n",
      "\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00036.jpg\n",
      "[-11.70071     5.788741    5.7991633]\n",
      "Final 0 Attempt Test\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00042.jpg\n",
      "[-14.1620245   6.033169    7.995866 ]\n",
      "Final 0 Attempt Test\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00054.jpg\n",
      "[-13.790673    5.6632776   7.9932075]\n",
      "Final 1 Attempt Test\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00058.jpg\n",
      "[-13.207197    5.6710014   7.4216514]\n",
      "Final 2 Attempt Test\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00083.jpg\n",
      "[-10.997107    5.3762407   5.541262 ]\n",
      "Final 3 Attempt Test\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00084.jpg\n",
      "[-11.07714     5.5543594   5.4374723]\n",
      "Final 4 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00039.jpg\n",
      "[-12.922286    5.7745943   7.015357 ]\n",
      "Final 5 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00007.jpg\n",
      "[-10.9478655   5.523552    5.2895713]\n",
      "Final 6 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00076.jpg\n",
      "[-11.044787    5.864534    5.0743737]\n",
      "Final 7 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00055.jpg\n",
      "[-13.050855    5.4573326   7.4870872]\n",
      "Final 8 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00070.jpg\n",
      "[-11.611014    5.580512    5.9674354]\n",
      "Final 9 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00013.jpg\n",
      "[-11.047298    5.1760354   5.74289  ]\n",
      "Final 10 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00082.jpg\n",
      "[-11.243196    5.5749416   5.5841594]\n",
      "Final 11 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00024.jpg\n",
      "[-11.637006    4.4759693   7.024192 ]\n",
      "Final 12 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00037.jpg\n",
      "[-12.908999    6.344163    6.4517984]\n",
      "Final 13 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00001.jpg\n",
      "[-10.552508    5.110402    5.3276887]\n",
      "Final 14 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00069.jpg\n",
      "[-10.886132    5.662642    5.1422925]\n",
      "Final 15 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00051.jpg\n",
      "[-14.371142   5.489683   8.726753]\n",
      "Final 16 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00085.jpg\n",
      "[-10.806755   5.492942   5.232144]\n",
      "Final 17 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00075.jpg\n",
      "[-11.119273    6.2497187   4.7894845]\n",
      "Final 18 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00071.jpg\n",
      "[-11.010029    6.2413797   4.6786723]\n",
      "Final 19 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00078.jpg\n",
      "[-11.425533   6.206974   5.112778]\n",
      "Final 20 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00044.jpg\n",
      "[-13.331027    6.2817574   6.919312 ]\n",
      "Final 21 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00046.jpg\n",
      "[-13.350981   6.059735   7.146604]\n",
      "Final 22 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00074.jpg\n",
      "[-11.757289    5.5077715   6.170541 ]\n",
      "Final 23 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00031.jpg\n",
      "[-11.8269615   5.1789255   6.5398684]\n",
      "Final 24 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00048.jpg\n",
      "[-13.684472    5.8252344   7.6867385]\n",
      "Final 25 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00009.jpg\n",
      "[-10.753236    4.93525     5.6748853]\n",
      "Final 26 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00002.jpg\n",
      "[-10.081824    5.161848    4.8115783]\n",
      "Final 27 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00018.jpg\n",
      "[-11.9515085   5.6438775   6.166509 ]\n",
      "Final 28 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00010.jpg\n",
      "[-10.48356    5.128907   5.22053 ]\n",
      "Final 29 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00012.jpg\n",
      "[-10.931872    5.0788445   5.7180676]\n",
      "Final 30 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00047.jpg\n",
      "[-13.893138    5.8992167   7.8255973]\n",
      "Final 31 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00029.jpg\n",
      "[-12.067256    5.199468    6.7523866]\n",
      "Final 32 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00025.jpg\n",
      "[-12.684311    5.2743306   7.2515006]\n",
      "Final 33 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00022.jpg\n",
      "[-11.316492    4.680212    6.4894614]\n",
      "Final 34 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00064.jpg\n",
      "[-12.235747    6.2455206   5.8829036]\n",
      "Final 35 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00034.jpg\n",
      "[-11.301034    5.1181993   6.06932  ]\n",
      "Final 36 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00027.jpg\n",
      "[-11.967721   5.157096   6.678903]\n",
      "Final 37 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00050.jpg\n",
      "[-13.875762   5.30616    8.410964]\n",
      "Final 38 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00061.jpg\n",
      "[-12.487256    4.603577    7.7933006]\n",
      "Final 39 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00003.jpg\n",
      "[-10.310633    5.3494887   4.8468266]\n",
      "Final 40 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00005.jpg\n",
      "[-10.812285    5.2800817   5.4088006]\n",
      "Final 41 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00028.jpg\n",
      "[-12.231208   5.112234   6.992932]\n",
      "Final 42 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00062.jpg\n",
      "[-13.674352    5.3251996   8.259204 ]\n",
      "Final 43 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00006.jpg\n",
      "[-10.831099    5.5942993   5.1015725]\n",
      "Final 44 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00026.jpg\n",
      "[-12.600748    5.051416    7.4168763]\n",
      "Final 45 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00067.jpg\n",
      "[-11.511035    4.5748806   6.8544793]\n",
      "Final 46 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00023.jpg\n",
      "[-11.109738    4.253367    6.7211413]\n",
      "Final 47 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00080.jpg\n",
      "[-11.277002    6.102995    5.0777736]\n",
      "Final 48 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00059.jpg\n",
      "[-12.994317    5.331049    7.5417657]\n",
      "Final 49 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00049.jpg\n",
      "[-13.675358   5.189182   8.323632]\n",
      "Final 50 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00038.jpg\n",
      "[-12.969732   6.314624   6.530314]\n",
      "Final 51 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00014.jpg\n",
      "[-11.061147    5.161769    5.7719336]\n",
      "Final 52 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00017.jpg\n",
      "[-10.525901    5.5618134   4.8294415]\n",
      "Final 53 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00068.jpg\n",
      "[-11.949984   5.361347   6.501883]\n",
      "Final 54 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00041.jpg\n",
      "[-13.925918    6.1290803   7.663241 ]\n",
      "Final 55 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00020.jpg\n",
      "[-12.410779    5.6595187   6.601131 ]\n",
      "Final 56 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00019.jpg\n",
      "[-12.988486    5.8796587   6.958586 ]\n",
      "Final 57 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00043.jpg\n",
      "[-14.040289    5.9549484   7.9547353]\n",
      "Final 58 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00079.jpg\n",
      "[-11.197883    5.8041925   5.305835 ]\n",
      "Final 59 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00065.jpg\n",
      "[-12.508016    5.387385    7.0318155]\n",
      "Final 60 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00077.jpg\n",
      "[-10.816013   5.971936   4.732619]\n",
      "Final 61 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00033.jpg\n",
      "[-11.215512    5.2708235   5.8328824]\n",
      "Final 62 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00066.jpg\n",
      "[-11.875707    4.5821047   7.211358 ]\n",
      "Final 63 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00032.jpg\n",
      "[-12.216216   5.437811   6.65642 ]\n",
      "Final 64 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00073.jpg\n",
      "[-11.471637    5.9860506   5.3876343]\n",
      "Final 65 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00072.jpg\n",
      "[-11.308235    5.7356105   5.4861827]\n",
      "Final 66 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00053.jpg\n",
      "[-13.804955    5.806506    7.8575234]\n",
      "Final 67 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00004.jpg\n",
      "[-10.567786    5.3525906   5.100537 ]\n",
      "Final 68 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00057.jpg\n",
      "[-12.32661     5.9237022   6.289147 ]\n",
      "Final 69 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00011.jpg\n",
      "[-10.9197445   5.1803765   5.597571 ]\n",
      "Final 70 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00035.jpg\n",
      "[-11.614606   5.32072    6.184192]\n",
      "Final 71 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00016.jpg\n",
      "[-10.571394    5.3742065   5.060427 ]\n",
      "Final 72 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00063.jpg\n",
      "[-12.760877   4.657181   8.027076]\n",
      "Final 73 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00040.jpg\n",
      "[-13.386075    6.0705147   7.170676 ]\n",
      "Final 74 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00081.jpg\n",
      "[-11.490606   5.668887   5.740272]\n",
      "Final 75 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00021.jpg\n",
      "[-11.767738    5.0386295   6.573342 ]\n",
      "Final 76 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00056.jpg\n",
      "[-12.680169    5.527449    7.0365033]\n",
      "Final 77 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00030.jpg\n",
      "[-11.571549   5.28154    6.181995]\n",
      "Final 78 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00015.jpg\n",
      "[-10.604993    5.2646527   5.211772 ]\n",
      "Final 79 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00052.jpg\n",
      "[-14.304139   5.727434   8.427758]\n",
      "Final 80 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00045.jpg\n",
      "[-13.351342    6.15624     7.0585613]\n",
      "Final 81 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00008.jpg\n",
      "[-10.878016    5.1404467   5.5949717]\n",
      "Final 82 Attempt Not Fall\n",
      "/data/w251fall/jpg/Fall/fall-25-front-urfall.val/img_00060.jpg\n",
      "[-12.26871    5.14184    7.007064]\n",
      "Final 83 Attempt Not Fall\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-81aba19ccd47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;31m#Modify number of classes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-81aba19ccd47>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_classes)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cap' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class GroupScale(object):\n",
    "    \"\"\" Rescales the input PIL.Image to the given 'size'.\n",
    "    'size' will be the size of the smaller edge.\n",
    "    For example, if height > width, then image will be\n",
    "    rescaled to (size * height / width, size)\n",
    "    size: size of the smaller edge\n",
    "    interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        self.worker = torchvision.transforms.Scale(size, interpolation)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "\n",
    "\n",
    "\n",
    "class GroupCenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.worker = torchvision.transforms.CenterCrop(size)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "\n",
    "\n",
    "\n",
    "class Stack(object):\n",
    "\n",
    "    def __init__(self, roll=False):\n",
    "        self.roll = roll\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        if img_group[0].mode == 'L':\n",
    "            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n",
    "        \n",
    "        elif img_group[0].mode == 'RGB':\n",
    "            if self.roll:\n",
    "                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n",
    "            else:\n",
    "                return np.concatenate(img_group, axis=2)\n",
    "\n",
    "\n",
    "\n",
    "class ToTorchFormatTensor(object):\n",
    "    \"\"\" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n",
    "    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] \"\"\"\n",
    "\n",
    "    def __init__(self, div=True):\n",
    "        self.div = div\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n",
    "        else:\n",
    "            # handle PIL Image\n",
    "            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
    "            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n",
    "            # put it from HWC to CHW format\n",
    "            # yikes, this transpose takes 80% of the loading time/CPU\n",
    "            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "        return img.float().div(255) if self.div else img.float()\n",
    "\n",
    "\n",
    "\n",
    "class GroupNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        rep_mean = self.mean * (tensor.size()[0] // len(self.mean))\n",
    "        rep_std = self.std * (tensor.size()[0] // len(self.std))\n",
    "\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, rep_mean, rep_std):\n",
    "            t.sub_(m).div_(s)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def process_output(idx_, history, num_classes):\n",
    "    # idx_: the output of current frame\n",
    "    # history: a list containing the history of predictions\n",
    "    if not REFINE_OUTPUT:\n",
    "        return idx_, history\n",
    "\n",
    "    max_hist_len = int((20/27)*num_classes) # max history buffer\n",
    "\n",
    "    # mask out illegal action\n",
    "    \n",
    "    if num_classes == 27:\n",
    "        if idx_ in [7, 8, 21, 22, 1, 3]:\n",
    "            idx_ = history[-1]\n",
    "\n",
    "        if idx_ == 0:\n",
    "            idx_ = 2\n",
    "\n",
    "    # use only single no action class\n",
    "    elif num_classes == 3: \n",
    "        if idx_ in [2]:\n",
    "            idx_ = history[-1]\n",
    "        \n",
    "        if idx_ == 0:\n",
    "            idx_ = 0\n",
    "    \n",
    "    # history smoothing\n",
    "\n",
    "    if idx_ != history[-1] and len(history) != 1:\n",
    "        if not (history[-1] == history[-2]): #  and history[-2] == history[-3]):\n",
    "            idx_ = history[-1]\n",
    "    \n",
    "\n",
    "    history.append(idx_)\n",
    "    history = history[-max_hist_len:]\n",
    "\n",
    "    return history[-1], history\n",
    "\n",
    "\n",
    "def get_categories(num_classes):\n",
    "\n",
    "    if num_classes == 27:\n",
    "        catigories = [\n",
    "        \"Doing other things\",  # 0\n",
    "        \"Drumming Fingers\",  # 1\n",
    "        \"No gesture\",  # 2\n",
    "        \"Pulling Hand In\",  # 3\n",
    "        \"Pulling Two Fingers In\",  # 4\n",
    "        \"Pushing Hand Away\",  # 5\n",
    "        \"Pushing Two Fingers Away\",  # 6\n",
    "        \"Rolling Hand Backward\",  # 7\n",
    "        \"Rolling Hand Forward\",  # 8\n",
    "        \"Shaking Hand\",  # 9\n",
    "        \"Sliding Two Fingers Down\",  # 10\n",
    "        \"Sliding Two Fingers Left\",  # 11\n",
    "        \"Sliding Two Fingers Right\",  # 12\n",
    "        \"Sliding Two Fingers Up\",  # 13\n",
    "        \"Stop Sign\",  # 14\n",
    "        \"Swiping Down\",  # 15\n",
    "        \"Swiping Left\",  # 16\n",
    "        \"Swiping Right\",  # 17\n",
    "        \"Swiping Up\",  # 18\n",
    "        \"Thumb Down\",  # 19\n",
    "        \"Thumb Up\",  # 20\n",
    "        \"Turning Hand Clockwise\",  # 21\n",
    "        \"Turning Hand Counterclockwise\",  # 22\n",
    "        \"Zooming In With Full Hand\",  # 23\n",
    "        \"Zooming In With Two Fingers\",  # 24\n",
    "        \"Zooming Out With Full Hand\",  # 25\n",
    "        \"Zooming Out With Two Fingers\"  # 26\n",
    "    ]\n",
    "\n",
    "    elif num_classes == 9: \n",
    "\n",
    "        catigories = [\"Fall\", \"SalsaSpin\", \"Taichi\", \"WallPushups\", \"WritingOnBoard\", \"Archery\", \"Hulahoop\", \"Nunchucks\", \"WalkingWithDog\"]\n",
    "    \n",
    "    elif num_classes == 10:\n",
    "\n",
    "        catigories = [\"Fall\", \"SalsaSpin\", \"Taichi\", \"WallPushups\", \"WritingOnBoard\", \"Archery\", \"Hulahoop\", \"Nunchucks\", \"WalkingWithDog\", \"test\"]\n",
    "\n",
    "    elif num_classes == 3 :\n",
    "\n",
    "        catigories = ['Fall', \"Not Fall\", \"Test\"]\n",
    "\n",
    "    elif num_classes == 2:\n",
    "\n",
    "        catigories = [\"Fall\", \"Not Fall\"]\n",
    "\n",
    "\n",
    "    return catigories\n",
    "\n",
    "\n",
    "def main(num_classes):\n",
    "\n",
    "\n",
    "    if num_classes not in [2, 3, 9, 10, 27]:\n",
    "        return \"Can only handle 2, 10, and 27 classes\"\n",
    "\n",
    "    else:\n",
    "        catigories = get_categories(num_classes)\n",
    "\n",
    "    cropping = torchvision.transforms.Compose([\n",
    "        GroupScale(256),\n",
    "        GroupCenterCrop(224),\n",
    "    ])\n",
    "\n",
    "\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        cropping,\n",
    "        Stack(roll=False),\n",
    "        ToTorchFormatTensor(div=True),\n",
    "        GroupNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "    torch_module = MobileNetV2(n_class=num_classes)\n",
    "    #print(torch_module.state_dict().keys())\n",
    "\n",
    "    if num_classes == 27:\n",
    "        if not os.path.exists(\"mobilenetv2_jester_online.pth.tar\"):  # checkpoint not downloaded\n",
    "            print('Downloading PyTorch checkpoint...')\n",
    "            url = 'https://hanlab.mit.edu/projects/tsm/models/mobilenetv2_jester_online.pth.tar'\n",
    "            urllib.request.urlretrieve(url, './mobilenetv2_jester_online.pth.tar')\n",
    "    \n",
    "\n",
    "        torch_module.load_state_dict(torch.load(\"mobilenetv2_jester_online.pth.tar\"))\n",
    "\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if num_classes == 9 or num_classes == 10:\n",
    "            model_new = torch.load(\"../../pretrained/9cat/ckpt.best.pth.tar\")\n",
    "    \n",
    "        elif num_classes == 2 or num_classes == 3:\n",
    "            #model_new = torch.load(\"../../pretrained/2cat/ckpt.best.pth.tar\")\n",
    "            model_new = torch.load(\"ckpt.best.pth.tar\")\n",
    "\n",
    "        # Fixing new model parameter mis-match\n",
    "        state_dict = model_new['state_dict']\n",
    "        #print(state_dict.keys())\n",
    "    \n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "\n",
    "        for k, v in state_dict.items():\n",
    "            #name = k[7:] # remove `module.`\n",
    "\n",
    "            if \"module.base_model.\" in k:\n",
    "                name = k.replace(\"module.base_model.\", \"\")\n",
    "\n",
    "                if \".net\" in name:\n",
    "                    name = name.replace(\".net\", \"\")\n",
    "\n",
    "\n",
    "            elif \"module.\" in k:\n",
    "                name = k.replace(\"module.new_fc.\", \"classifier.\")\n",
    "        \n",
    "\n",
    "            new_state_dict[name] = v\n",
    "\n",
    "        # load params\n",
    "        torch_module.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "    torch_module.eval()\n",
    "\n",
    "    #KH cap = cv2.VideoCapture(1)\n",
    "\n",
    "    # set a lower resolution for speed up\n",
    "    #KH cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)\n",
    "    #KH cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)\n",
    "    \n",
    "    # Load jpgs file names fully qualified\n",
    "    video_images_dir = '/data/w251fall/jpg/Fall/fall-25-front-urfall.val'\n",
    "    jpg_filenames = [video_images_dir + '/' + s for s in os.listdir(video_images_dir)]\n",
    "\n",
    "    \n",
    "    #full_screen = False\n",
    "    #KH cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)\n",
    "    #KH cv2.resizeWindow(WINDOW_NAME, 640, 480)\n",
    "    #KH cv2.moveWindow(WINDOW_NAME, 0, 0)\n",
    "    #KH cv2.setWindowTitle(WINDOW_NAME, WINDOW_NAME)\n",
    "\n",
    "\n",
    "    shift_buffer = [torch.zeros([1, 3, 56, 56]),\n",
    "                    torch.zeros([1, 4, 28, 28]),\n",
    "                    torch.zeros([1, 4, 28, 28]),\n",
    "                    torch.zeros([1, 8, 14, 14]),\n",
    "                    torch.zeros([1, 8, 14, 14]),\n",
    "                    torch.zeros([1, 8, 14, 14]),\n",
    "                    torch.zeros([1, 12, 14, 14]),\n",
    "                    torch.zeros([1, 12, 14, 14]),\n",
    "                    torch.zeros([1, 20, 7, 7]),\n",
    "                    torch.zeros([1, 20, 7, 7])]\n",
    "\n",
    "\n",
    "    t = None    \n",
    "    index = 0\n",
    "    idx = 0\n",
    "    history = [2]\n",
    "    history_logit = []\n",
    "    history_timing = []\n",
    "    i_frame = -1\n",
    "\n",
    "    #KH while True:\n",
    "    for jpg_filename in jpg_filenames:\n",
    "        \n",
    "        print(jpg_filename)\n",
    "        img = cv2.imread(jpg_filename)\n",
    "        \n",
    "        i_frame += 1\n",
    "        #KH _, img = cap.read()  # (480, 640, 3) 0 ~ 255\n",
    "\n",
    "        if i_frame % 1 == 0:\n",
    "            t1 = time.time()\n",
    "            img_tran = transform([Image.fromarray(img).convert('RGB')])\n",
    "            input_var = torch.autograd.Variable(img_tran.view(1, 3, img_tran.size(1), img_tran.size(2)))\n",
    "\n",
    "            #prediction = torch_module(input_var, *shift_buffer) #demo mobilenet\n",
    "            prediction = torch_module(input_var) #arch mobilenet\n",
    "\n",
    "\n",
    "            feat, shift_buffer = prediction[0], prediction[1:]\n",
    "\n",
    "\n",
    "            if SOFTMAX_THRES > 0:\n",
    "\n",
    "                feat_np = feat.detach().numpy().reshape(-1)\n",
    "                feat_np -= feat_np.max()\n",
    "\n",
    "                softmax = np.exp(feat_np) / np.sum(np.exp(feat_np))\n",
    "\n",
    "                print(max(softmax))\n",
    "        \n",
    "                if max(softmax) > SOFTMAX_THRES:\n",
    "                    idx_ = np.argmax(feat.detach().numpy(), axis=1)[0]\n",
    "        \n",
    "                else:\n",
    "                    idx_ = idx\n",
    "    \n",
    "            else:\n",
    "                print(feat.detach().numpy())\n",
    "                #idx_ = np.argmax(feat.detach().numpy(), axis=1)[0] For demo mobilenet\n",
    "                idx_ = np.argmax(feat.detach().numpy()) # For archnet mobilenet\n",
    "\n",
    "\n",
    "            if HISTORY_LOGIT:\n",
    "                history_logit.append(feat.detach().numpy())\n",
    "                history_logit = history_logit[-int(12/27*num_classes):]\n",
    "                avg_logit = sum(history_logit)\n",
    "                #idx_ = np.argmax(avg_logit, axis=1)[0] For demo mobilenet\n",
    "                idx_ = np.argmax(avg_logit)  #For archnet mobilenet\n",
    "\n",
    "            idx, history = process_output(idx_, history, num_classes)\n",
    "            \n",
    "\n",
    "            t2 = time.time()\n",
    "            print(f\"Final {index} Attempt {catigories[idx]}\")\n",
    "\n",
    "            \n",
    "            current_time = t2 - t1\n",
    "\n",
    "        \n",
    "        img = cv2.resize(img, (640, 480))\n",
    "        img = img[:, ::-1]\n",
    "        height, width, _ = img.shape\n",
    "        label = np.zeros([height // 10, width, 3]).astype('uint8') + 255\n",
    "\n",
    "        #KH cv2.putText(label, 'Prediction: ' + catigories[idx], (0, int(height / 16)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "        #KH cv2.putText(label, '{:.1f} Vid/s'.format(1 / current_time), (width - 170, int(height / 16)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "\n",
    "        #KH img = np.concatenate((img, label), axis=0)\n",
    "        #KH cv2.imshow(WINDOW_NAME, img)\n",
    "\n",
    "        #KH key = cv2.waitKey(1)\n",
    "\n",
    "        #KH if key & 0xFF == ord('q') or key == 27:  # exit\n",
    "        #KH     break\n",
    "        \n",
    "        #KH elif key == ord('F') or key == ord('f'):  # full screen\n",
    "        #KH     print('Changing full screen option!')\n",
    "            \n",
    "        #KH     full_screen = not full_screen\n",
    "            \n",
    "        #KH     if full_screen:\n",
    "        #KH         print('Setting FS!!!')\n",
    "        #KH         cv2.setWindowProperty(WINDOW_NAME, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "        #KH     \n",
    "        #KH     else:\n",
    "        #KH         cv2.setWindowProperty(WINDOW_NAME, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_NORMAL)\n",
    "\n",
    "\n",
    "        if t is None:\n",
    "            t = time.time()\n",
    "        \n",
    "        else:\n",
    "            nt = time.time()\n",
    "            index += 1\n",
    "            t = nt\n",
    "\n",
    "\n",
    "    #KH cap.release()\n",
    "    #KH cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting... \\n\")\n",
    "\n",
    "    SOFTMAX_THRES = 0\n",
    "    HISTORY_LOGIT = True\n",
    "    REFINE_OUTPUT = True\n",
    "    WINDOW_NAME = \"GESTURE CAPTURE\"\n",
    "\n",
    "    #Modify number of classes here\n",
    "    main(3)\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
